name: Free Web Scraping Pipeline with Zillow API

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'Starting URL to scrape'
        required: true
        default: 'https://www.zillow.com/homedetails/example-property/'
      extract_type:
        description: 'What to extract'
        required: true
        default: 'entities_relationships'
        type: choice
        options:
        - 'entities_relationships'
        - 'links_only'
        - 'text_summary'
      max_pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '3'
  
  schedule:
    - cron: '0 6 * * *'
  
  push:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - '.github/workflows/scraping.yml'

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install requests beautifulsoup4 pydantic
        
    - name: Create output directories
      run: |
        mkdir -p output
        mkdir -p web_interface/data
        
    - name: Run scraping pipeline with API support
      env:
        START_URL: ${{ github.event.inputs.start_url || 'https://www.zillow.com/' }}
        EXTRACT_TYPE: ${{ github.event.inputs.extract_type || 'entities_relationships' }}
        MAX_PAGES: ${{ github.event.inputs.max_pages || '3' }}
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      run: |
        cd scripts
        echo "🔑 API Key Status: $([[ -n "$RAPIDAPI_KEY" ]] && echo "✅ Available" || echo "❌ Missing")"
        python free_scraper.py
        
    - name: Convert to Cypher
      run: |
        cd scripts
        python convert_to_cypher.py
        
    - name: Show results preview
      run: |
        echo "=== 📊 SCRAPING RESULTS ==="
        echo "Files created:"
        ls -la output/
        echo ""
        echo "=== 📄 JSON Preview ==="
        head -30 output/*.json | head -20
        echo ""
        echo "=== 🗄️ Cypher Preview ==="
        head -10 output/*.cypher | head -10
        
    - name: Prepare web interface data
      run: |
        # Copy results to web interface for display
        cp output/*.json web_interface/data/ 2>/dev/null || true
        
        # Create enhanced status file
        echo "{
          \"status\": \"success\",
          \"timestamp\": \"$(date -Iseconds)\",
          \"run_id\": \"${{ github.run_number }}\",
          \"url_scraped\": \"${{ github.event.inputs.start_url || 'https://zillow.com/' }}\",
          \"api_enabled\": $([[ -n \"${{ secrets.RAPIDAPI_KEY }}\" ]] && echo \"true\" || echo \"false\"),
          \"files_available\": $(ls output/*.json 2>/dev/null | wc -l),
          \"extraction_type\": \"${{ github.event.inputs.extract_type || 'entities_relationships' }}\"
        }" > web_interface/data/status.json
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          output/
          web_interface/data/
        retention-days: 30
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      if: github.ref == 'refs/heads/main'
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./web_interface
