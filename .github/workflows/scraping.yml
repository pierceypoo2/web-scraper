name: Free Web Scraping Pipeline

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'Starting URL to scrape'
        required: true
        default: 'https://crawl4ai.com/'
      extract_type:
        description: 'What to extract'
        required: true
        default: 'entities_relationships'
        type: choice
        options:
        - 'entities_relationships'
        - 'links_only'
        - 'text_summary'
      max_pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '3'
  
  schedule:
    - cron: '0 6 * * *'
  
  push:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - '.github/workflows/scraping.yml'

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install requests beautifulsoup4 pydantic
        
    - name: Create output directories
      run: |
        mkdir -p output
        mkdir -p web_interface/data
        
    - name: Run scraping pipeline
      env:
        START_URL: ${{ github.event.inputs.start_url || 'https://crawl4ai.com/' }}
        EXTRACT_TYPE: ${{ github.event.inputs.extract_type || 'entities_relationships' }}
        MAX_PAGES: ${{ github.event.inputs.max_pages || '3' }}
      run: |
        cd scripts
        python free_scraper.py
        
    - name: Convert to Cypher
      run: |
        cd scripts
        python convert_to_cypher.py
        
    - name: Prepare web interface data
      run: |
        # Copy results to web interface for display
        cp output/*.json web_interface/data/ 2>/dev/null || true
        
        # Create status file for the website
        echo "{
          \"status\": \"success\",
          \"timestamp\": \"$(date -Iseconds)\",
          \"run_id\": \"${{ github.run_number }}\",
          \"url_scraped\": \"${{ github.event.inputs.start_url || 'https://crawl4ai.com/' }}\",
          \"files_available\": $(ls output/*.json 2>/dev/null | wc -l)
        }" > web_interface/data/status.json
        
    - name: Upload artifacts (FIXED VERSION)
      uses: actions/upload-artifact@v4
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          output/
          web_interface/data/
        retention-days: 30
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      if: github.ref == 'refs/heads/main'
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./web_interface
